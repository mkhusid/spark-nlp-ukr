{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredStreaming\") \\\n",
    "    .config('spark.executor.memory', '16g') \\\n",
    "    .config('spark.driver.memory', '16g') \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, CountVectorizer, IDF, HashingTF\n",
    "from pyspark.ml import feature\n",
    "import spacy\n",
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормалізація та попередня обробка даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, functions as F\n",
    "\n",
    "class UkrainianProcessor:\n",
    "    def __init__(self, raw_text: DataFrame):\n",
    "        self.regexp_letters = r'[^\\w а-яїєіґ]'\n",
    "        self.raw_text = raw_text\n",
    "    \n",
    "    def cleanup_data(self):\n",
    "        # Провести очищення текстових даних від стоп-слів/тегів/розмітки;\n",
    "        self.clean_text = self.raw_text \\\n",
    "            .withColumn(\"title\", F.regexp_replace(F.lower(F.col(\"Title\")), self.regexp_letters, \"\")) \\\n",
    "            .withColumn(\"body\", F.regexp_replace(F.lower(F.col(\"Body\")), self.regexp_letters, \"\"))\n",
    "\n",
    "        self.clean_text.show(5)\n",
    "        return self.clean_text\n",
    "    \n",
    "    def tokenize_text(self, clean_text, input_column):\n",
    "        # Виконати токенізацію текстових елементів; \n",
    "        self.input_column = input_column\n",
    "        \n",
    "        tokenizer = feature.Tokenizer(\n",
    "            inputCol=self.input_column, outputCol=f\"{self.input_column}_tokens\")\n",
    "        tokenized = tokenizer.transform(clean_text)\n",
    "\n",
    "        # Видалення стоп-слів\n",
    "        stopwords_remover = StopWordsRemover().setLocale('uk_UA').setInputCol(\n",
    "            f\"{self.input_column}_tokens\").setOutputCol(f\"{self.input_column}_filtered\")\n",
    "        self.filtered = stopwords_remover.transform(tokenized)\n",
    "\n",
    "        print('Locale:', stopwords_remover.getLocale())\n",
    "        print('Tokenized & Filtered:')\n",
    "        self.filtered.printSchema()\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def lemmatize_text(self):\n",
    "        # Провести лематизацію текстових елементів (Spacy). Зберегти результат в окремий файл.\n",
    "        nlp = spacy.load(\"uk_core_news_sm\")\n",
    "        self.stop_words = nlp.Defaults.stop_words   \n",
    "                \n",
    "        stop_words = [*self.stop_words, ' ']         \n",
    "        @F.udf(\"array<string>\")\n",
    "        def lemmatize_udf(tokens):\n",
    "            return [token.lemma_ for token in nlp(\" \".join(tokens)) if token.lemma_ not in stop_words]\n",
    "\n",
    "        self.lemmatized = self.filtered.withColumn( \n",
    "            f\"lemmatized_{self.input_column}\",\n",
    "            lemmatize_udf(F.col(f\"{self.input_column}_filtered\")) \n",
    "        )\n",
    "        \n",
    "        print('Lemmatized:')\n",
    "        self.lemmatized.printSchema()\n",
    "        return self       \n",
    "    \n",
    "    def create_bow(self):\n",
    "        # Створити Bag of Words для всіх нормалізованих слів. Зберегти результат в окремий файл.\n",
    "\n",
    "        count_vectorizer = CountVectorizer(\n",
    "            inputCol=f\"lemmatized_{self.input_column}\", outputCol=f\"{self.input_column}_features\")\n",
    "        cv_model = count_vectorizer.fit(self.lemmatized.limit(100))\n",
    "        self.bag_of_words = cv_model.transform(self.lemmatized.limit(100))\n",
    "        self.words = cv_model.vocabulary\n",
    "        \n",
    "        return self.bag_of_words\n",
    "    \n",
    "    def measure_tf_idf(self, bow, input_column):\n",
    "        idf = IDF(inputCol=f\"{input_column}_features\",\n",
    "                outputCol=f\"{input_column}_tf_idf\", minDocFreq=10)\n",
    "        self.tf_idf = idf.fit(bow).transform(bow)\n",
    "        return self.tf_idf\n",
    "    \n",
    "    def save_intermediate_results(self):\n",
    "        self.lemmatized.toPandas().to_csv('../data/lemmatized_tokens.csv')\n",
    "        self.bag_of_words.toPandas().to_csv('../data/bag_of_words.csv')\n",
    "                \n",
    "    \n",
    "    def preprocessing(self, input_column):\n",
    "        clean = self.cleanup_data()\n",
    "        embedded = self.tokenize_text(clean, input_column).lemmatize_text().create_bow()\n",
    "        self.save_intermediate_results()\n",
    "        \n",
    "        return embedded\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = './data/'\n",
    "\n",
    "ukr_text_data = spark.read.option(\"encoding\", \"UTF-8\").option(\"header\", \"true\").csv(f\"{basepath}/ukr_text.csv\")\n",
    "ukr_text_data = ukr_text_data.select('Title', 'Body')\n",
    "\n",
    "ukr_text_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukr_pr = UkrainianProcessor(ukr_text_data)    \n",
    "preprocessed_body = ukr_pr.preprocessing('body')\n",
    "tf_idf_body = ukr_pr.measure_tf_idf(preprocessed_body, 'body')\n",
    "\n",
    "tf_idf_body.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = ukr_pr.words\n",
    "\n",
    "@F.udf\n",
    "def get_popular_score(tf_idf):\n",
    "    values = tf_idf.toArray()\n",
    "    highest = float(max(values))\n",
    "    return highest\n",
    "\n",
    "@F.udf\n",
    "def get_popular_word(tf_idf, tf_idf_score):\n",
    "    global words_list\n",
    "    values = tf_idf.toArray()\n",
    "\n",
    "    highest_idx = list(values).index(float(tf_idf_score))\n",
    "    return words_list[highest_idx]\n",
    "\n",
    "words_rating = tf_idf_body.select('lemmatized_body', 'body_tf_idf'\n",
    "    ).withColumn('word_score', get_popular_score('body_tf_idf').cast('double')\n",
    "    ).withColumn('word', get_popular_word('body_tf_idf', 'word_score')\n",
    ").groupBy('word','word_score').max('word_score').orderBy(F.desc('word_score'))\n",
    "    \n",
    "    \n",
    "words_rating.printSchema()    \n",
    "words_rating.select('word', 'word_score').dropDuplicates(['word']).limit(10).show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Term Frequency: is a scoring of the frequency of the word in the current document.\n",
    "Inverse Document Frequency: is a scoring of how rare the word is across documents.\n",
    "'''\n",
    "\n",
    "processed_body = tf_idf_body.select('title', 'body', 'body_tf_idf')\n",
    "ukr_pr_title = UkrainianProcessor(processed_body)    \n",
    "\n",
    "preprocessed_title = ukr_pr_title.preprocessing('title')\n",
    "tf_idf_title = ukr_pr_title.measure_tf_idf(preprocessed_title, 'title')\n",
    "\n",
    "all_processed = tf_idf_title.select('title', 'title_tf_idf', 'body', 'body_tf_idf')\n",
    "all_processed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Порахувати косинусну подібність між полями датасету title та body\n",
    "\n",
    "@F.udf\n",
    "def cosine_similarity(v1, v2):\n",
    "    size = max(len(v1), len(v2))\n",
    "    vec1_array = np.zeros(size)\n",
    "    vec2_array = np.zeros(size)\n",
    "    \n",
    "    vec1_array[:len(v1)] = np.array(v1)\n",
    "    vec2_array[:len(v2)] = np.array(v2)\n",
    "    \n",
    "    res = np.dot(vec1_array, vec2_array) / (np.linalg.norm(vec1_array) * np.linalg.norm(vec2_array))\n",
    "    return float(res)\n",
    "\n",
    "similarity = all_processed.withColumn(\"cosine_similarity\", cosine_similarity( F.col(\"body_tf_idf\"), F.col(\"title_tf_idf\")))\n",
    "\n",
    "similarity.where('cosine_similarity != \"NaN\" ').select('cosine_similarity', 'title', 'body').orderBy(F.desc('cosine_similarity')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Інтелектуальний аналіз текстів з використанням LLM моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import Tokenizer, StopWordsCleaner\n",
    "from sparknlp.annotator import WordEmbeddingsModel\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def make_embedding(input_data, column, clean_stop_words=False):\n",
    "    documentAssembler = DocumentAssembler() \\\n",
    "        .setInputCol(column) \\\n",
    "        .setOutputCol(f\"{column}_document\")\n",
    "        \n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols(f\"{column}_document\") \\\n",
    "        .setOutputCol(f\"{column}_token\")\n",
    "        \n",
    "    stopWords = StopWordsCleaner.pretrained(\"stopwords_iso\",\"uk\") \\\n",
    "        .setInputCols([f\"{column}_token\"]) \\\n",
    "        .setOutputCol(f\"{column}_token\")\n",
    "\n",
    "\n",
    "    embeddings = WordEmbeddingsModel.pretrained(\"w2v_cc_300d\", \"uk\") \\\n",
    "        .setInputCols([f\"{column}_document\", f\"{column}_token\"]) \\\n",
    "        .setOutputCol(f\"{column}_embeddings\")\n",
    "        \n",
    "    if clean_stop_words:\n",
    "        pipeline = Pipeline(stages=[documentAssembler,tokenizer, stopWords, embeddings])\n",
    "    else:\n",
    "        pipeline = Pipeline(stages=[documentAssembler,tokenizer, embeddings])\n",
    "\n",
    "    model = pipeline.fit(input_data)\n",
    "    embeddings = model.transform(input_data)\n",
    "\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = UkrainianProcessor(ukr_text_data).cleanup_data()\n",
    "\n",
    "embedded_body = make_embedding(cleaned_text, 'body')\n",
    "embedded_title = make_embedding(cleaned_text, 'title')\n",
    "\n",
    "joined = embedded_body.alias('text').join(embedded_title.alias('name'),\n",
    "    F.col('text.title') == F.col('name.title'))\n",
    "\n",
    "joined = joined.select('text.title', 'text.body',\n",
    "        F.col('body_embeddings.embeddings').alias('body_embeddings'), \n",
    "        F.col('title_embeddings.embeddings').alias('title_embeddings')\n",
    "    )\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Порахувати косинусну подібність між полями датасету title та body за посиланням.\n",
    "\n",
    "@F.udf(\"float\")\n",
    "def cosine_similarity(v1, v2):\n",
    "    v1, v2 = v1[0], v2[0]\n",
    "    size = max(len(v1), len(v2))\n",
    "    vec1_array = np.zeros(size)\n",
    "    vec2_array = np.zeros(size)\n",
    "    \n",
    "    vec1_array[:len(v1)] = np.array(v1)\n",
    "    vec2_array[:len(v2)] = np.array(v2)\n",
    "    \n",
    "    res = np.dot(vec1_array, vec2_array) / (np.linalg.norm(vec1_array) * np.linalg.norm(vec2_array))\n",
    "    return float(res)\n",
    "\n",
    "similarity_nlp = joined.withColumn(\"cosine_similarity\", cosine_similarity( F.col(\"body_embeddings\"), F.col(\"title_embeddings\")))\n",
    "\n",
    "similarity_nlp = similarity_nlp.filter(~F.isnan(\"cosine_similarity\") & ~F.isnull(\"cosine_similarity\")).orderBy(F.desc('cosine_similarity'))\n",
    "similarity_nlp.select('cosine_similarity', 'title', 'body').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Порахувати кількість дублів в наборі даних з різними косинусними подібностями (наприклад > 0.7, > 0.99); \n",
    "def count_duplicates(dataframe: DataFrame, thresholds):  \n",
    "    for threshold in thresholds:\n",
    "        dp_count = dataframe.filter(f\"cosine_similarity > {threshold}\").count()\n",
    "        print(f\"Кількість дублікатів (cosine > {threshold}): {dp_count}\")\n",
    "\n",
    "\n",
    "count_duplicates(similarity_nlp, [0.7, 0.9])\n",
    "\n",
    "# Вивести топ-10 унікальних дата поїнтів.\n",
    "similarity_nlp.select('cosine_similarity','title', 'body').filter(\n",
    "    F.col(\"cosine_similarity\") < 0.01).dropDuplicates().limit(10).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import VectorUDT, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "import re\n",
    "\n",
    "\n",
    "embedded_lda = make_embedding(cleaned_text, 'body', clean_stop_words=True)\n",
    "\n",
    "\n",
    "def collect_vocabulary(embedded_df):\n",
    "    embeddings_df = embedded_df.select(\"body_embeddings\").toPandas()\n",
    "    vocab = []\n",
    "    for row in embeddings_df[\"body_embeddings\"]:\n",
    "        vocab.extend([emb.metadata['token'] for emb in row])\n",
    "    return vocab\n",
    "\n",
    "vocabulary = collect_vocabulary(embedded_lda)\n",
    "\n",
    "\n",
    "# Підготовка векторів слів для LDA\n",
    "@F.udf(VectorUDT())\n",
    "def array_of_struct_to_vector(features):    \n",
    "    words_dict = {}\n",
    "\n",
    "    for struct in features:\n",
    "        if re.match(r'[^\\d]+', struct['metadata']['token']):\n",
    "            idx = vocabulary.index(struct['metadata']['token'])\n",
    "            words_dict[idx] = np.sum(struct['embeddings'])\n",
    "        \n",
    "    vect = SparseVector(len(vocabulary), words_dict)\n",
    "    return vect\n",
    "\n",
    "processed_lda = embedded_lda.withColumn(\"features\", array_of_struct_to_vector(embedded_lda[\"body_embeddings\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_lda(num_topics, max_iterations, vocabulary, vectorized_docs, features_col=\"features\"):\n",
    "\n",
    "    lda = LDA(k=num_topics, maxIter=max_iterations, optimizer=\"em\") \\\n",
    "        .setFeaturesCol(features_col) \\\n",
    "        .setTopicDistributionCol(\"topic_distribution\")\n",
    "\n",
    "    lda_model = lda.fit(vectorized_docs)\n",
    "\n",
    "    @F.udf\n",
    "    def parse_words(topic):\n",
    "        return [vocabulary[i] for i in topic]\n",
    "\n",
    "    topics = lda_model.describeTopics()\n",
    "    topics_with_words = topics.withColumn('words', parse_words('termIndices'))\n",
    "    topics_with_words.select('topic', 'words').show(truncate=False)\n",
    "    \n",
    "# Build LDA for Word2Vec\n",
    "build_lda(5, 10, vocabulary, processed_lda.limit(100))\n",
    "\n",
    "# Build LDA for CountVectorizer\n",
    "build_lda(5, 10, ukr_pr.words, preprocessed_body, 'body_features')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
